<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Discrete Opinion Tree Induction for Aspect-based Sentiment Analysis</title>
      <link href="/2023/03/25/dotGCN/"/>
      <url>/2023/03/25/dotGCN/</url>
      
        <content type="html"><![CDATA[<blockquote><p>2022 ACL <a href="dotGCN.pdf">Paper</a></p></blockquote><hr><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><ol><li>低资源语言可能没有解析器得到依存树</li><li>现有的依存树没有针对aspect进行优化</li></ol><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>预训练语言模型的注意力分数作为初始化<br>根据注意力分数生成树结构<br>使用强化学习优化生成树的结构</p><img src="dotGCN1.png" width="50%" height="50%"/><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><img src="dotGCN2.png" width="50%" height="50%"/>]]></content>
      
      
      <categories>
          
          <category> ABSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tree </tag>
            
            <tag> reinforcement learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Contrastive Cross-Channel Data Augmentation Framework for Aspect-Based Sentiment Analysis</title>
      <link href="/2023/03/24/C3DA/"/>
      <url>/2023/03/24/C3DA/</url>
      
        <content type="html"><![CDATA[<h2 id="A-Contrastive-Cross-Channel-Data-Augmentation-Framework-for-Aspect-Based-Sentiment-Analysis"><a href="#A-Contrastive-Cross-Channel-Data-Augmentation-Framework-for-Aspect-Based-Sentiment-Analysis" class="headerlink" title="A Contrastive Cross-Channel Data Augmentation Framework for Aspect-Based Sentiment Analysis"></a>A Contrastive Cross-Channel Data Augmentation Framework for Aspect-Based Sentiment Analysis</h2><blockquote><p>2022 COLING <a href="C3DA.pdf">Paper</a></p></blockquote><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ol><li>数据增强提高预测多方面句子的鲁棒性</li><li>使用预训练语言模型生成</li></ol><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>方面和情感双通道 对比学习</p><img src="C3DA1.png" width="50%" height="50%"/><h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><img src="C3DA2.png" width="50%" height="50%"/>]]></content>
      
      
      <categories>
          
          <category> ABSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> contrastive learning </tag>
            
            <tag> DA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BiSyn-GAT+ Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis</title>
      <link href="/2023/03/24/BiSyn-GAT/"/>
      <url>/2023/03/24/BiSyn-GAT/</url>
      
        <content type="html"><![CDATA[<h2 id="BiSyn-GAT-Bi-Syntax-Aware-Graph-Attention-Network-for-Aspect-based-Sentiment-Analysis"><a href="#BiSyn-GAT-Bi-Syntax-Aware-Graph-Attention-Network-for-Aspect-based-Sentiment-Analysis" class="headerlink" title="BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis"></a>BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis</h2><blockquote><p>2022 ACL(Findings) <a href="BiSyn-GAT.pdf">Paper</a></p></blockquote><hr><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ol><li>位置和依存树都不一定能将方面及其对应上下文对齐 <strong>(intra-aspect)</strong></li><li>无法对多个方面的复杂关系建模 <strong>(inter-aspect)</strong></li></ol><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ol><li>成分树将复杂句子划分成多个子句，对齐方面及其上下文</li><li>利用依存树的句法信息和成分树的层次结构，对方面上下文和方面之间的关系进行建模</li></ol><img src="BiSyn-GAT1.png" width="50%" height="50%"/><h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><img src="BiSyn-GAT2.png" width="50%" height="50%"/>]]></content>
      
      
      <categories>
          
          <category> ABSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> constituency </tag>
            
            <tag> dependency </tag>
            
            <tag> GAT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention and Lexicon Regularized LSTM for Aspect-based Sentiment Analysis</title>
      <link href="/2023/03/19/ATLX/"/>
      <url>/2023/03/19/ATLX/</url>
      
        <content type="html"><![CDATA[<blockquote><p>2022 ACL(Findings) <a href="ATLX.pdf">Paper</a></p></blockquote><hr><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><ol><li>端到端深度神经网络在训练数据少时缺乏灵活性，注意力机制可能会过度关注句子的特定部分，而忽略了为判断极性提供关键信息的位置 <strong>(over-attention)</strong></li><li>作者提出利用词典信息这一简单有效方法，使模型灵活性和鲁棒性更强，另外，作者对注意力向量正则化，使模型获得对句子不同部分有更多的关注点 <strong>(lexicon+regularize)</strong></li></ol><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><ol><li>Lexicon feature: 合并MPQA, Opinion Lexicon, Opener和Vader四个词典，SentiWordNet因引入噪音而被移除。将每个词在不同词典的最大极性分数拼接，作为该词的极性分数向量 例如: adorable [1.0, 1.0, 1.0, 0.55] MPQA(1.0), Opener(1.0), Opinion Lexicon(1.0) and Vader(0.55)</li><li>Attention Regularization: 为了减小注意力过拟合，作者在损失函数加入对注意力分数的正则化R(α)，提出了两种计算方法：标准差和负信息熵</li></ol><img src="ATLX1.png" width="50%" height="50%"/><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><img src="ATLX2.png" width="50%" height="50%"/>]]></content>
      
      
      <categories>
          
          <category> ABSA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lexicon </tag>
            
            <tag> attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/03/18/hello-world/"/>
      <url>/2023/03/18/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
